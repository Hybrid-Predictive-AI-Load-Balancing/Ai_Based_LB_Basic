{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48c1b4ee",
   "metadata": {},
   "source": [
    "# üöÄ Enhanced Training Data Generator + Model Training\n",
    "\n",
    "This notebook will:\n",
    "1. Generate comprehensive training data with ALL spike types\n",
    "2. Train XGBoost models on the enhanced data\n",
    "3. Save .pkl files ready for download\n",
    "\n",
    "**Run all cells in order!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c90db3",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a882d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost scikit-learn pandas numpy joblib -q\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e093b7",
   "metadata": {},
   "source": [
    "## üé≤ Step 2: Generate Enhanced Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d23b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"üöÄ Enhanced Training Data Generator\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Configuration\n",
    "TOTAL_DURATION = 10 * 24 * 3600  # 10 days\n",
    "BASE_PODS = 5\n",
    "\n",
    "print(f\"‚è±Ô∏è  Total duration: {TOTAL_DURATION // 86400} days\")\n",
    "print(f\"üìä Expected rows: ~{TOTAL_DURATION:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77b396c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traffic Pattern Generators\n",
    "\n",
    "def daily_baseline_pattern(t):\n",
    "    \"\"\"Realistic daily traffic pattern\"\"\"\n",
    "    hour = (t % 86400) / 3600\n",
    "    \n",
    "    # Morning ramp-up\n",
    "    morning_boost = 20 * ((hour - 6) / 3) if 6 <= hour < 9 else 0\n",
    "    # Lunch spike\n",
    "    lunch_boost = 30 if 12 <= hour < 13 else 0\n",
    "    # Evening peak\n",
    "    evening_boost = 40 * np.sin(np.pi * (hour - 17) / 3) if 17 <= hour < 20 else 0\n",
    "    \n",
    "    base = 60 + 20 * np.sin(2 * np.pi * t / 86400)\n",
    "    noise = np.random.normal(0, 5)\n",
    "    \n",
    "    return max(10, base + morning_boost + lunch_boost + evening_boost + noise)\n",
    "\n",
    "\n",
    "def generate_sudden_spikes(duration, count=150):\n",
    "    \"\"\"Sudden sharp spikes (like test case)\"\"\"\n",
    "    spikes = []\n",
    "    positions = np.random.choice(duration, count, replace=False)\n",
    "    for pos in positions:\n",
    "        start = int(pos)\n",
    "        spike_duration = np.random.randint(30, 80)\n",
    "        magnitude = np.random.uniform(3.5, 5.0)\n",
    "        spikes.append(('sudden', start, spike_duration, magnitude))\n",
    "    return spikes\n",
    "\n",
    "\n",
    "def generate_gradual_ramps(duration, count=80):\n",
    "    \"\"\"Gradual ramp-ups and downs\"\"\"\n",
    "    ramps = []\n",
    "    positions = np.random.choice(duration, count, replace=False)\n",
    "    for pos in positions:\n",
    "        start = int(pos)\n",
    "        ramp_duration = np.random.randint(300, 600)\n",
    "        peak_magnitude = np.random.uniform(2.0, 3.5)\n",
    "        ramps.append(('gradual', start, ramp_duration, peak_magnitude))\n",
    "    return ramps\n",
    "\n",
    "\n",
    "def generate_sustained_bursts(duration, count=60):\n",
    "    \"\"\"Long sustained high load\"\"\"\n",
    "    bursts = []\n",
    "    positions = np.random.choice(duration, count, replace=False)\n",
    "    for pos in positions:\n",
    "        start = int(pos)\n",
    "        burst_duration = np.random.randint(600, 1800)\n",
    "        magnitude = np.random.uniform(3.0, 4.5)\n",
    "        bursts.append(('sustained', start, burst_duration, magnitude))\n",
    "    return bursts\n",
    "\n",
    "\n",
    "def generate_oscillating_patterns(duration, count=40):\n",
    "    \"\"\"Oscillating traffic\"\"\"\n",
    "    oscillations = []\n",
    "    positions = np.random.choice(duration, count, replace=False)\n",
    "    for pos in positions:\n",
    "        start = int(pos)\n",
    "        pattern_duration = np.random.randint(400, 800)\n",
    "        base_magnitude = np.random.uniform(1.5, 2.5)\n",
    "        oscillations.append(('oscillating', start, pattern_duration, base_magnitude))\n",
    "    return oscillations\n",
    "\n",
    "\n",
    "def generate_cascading_spikes(duration, count=30):\n",
    "    \"\"\"Sequential spikes getting closer\"\"\"\n",
    "    cascades = []\n",
    "    positions = np.random.choice(duration - 1000, count, replace=False)\n",
    "    for pos in positions:\n",
    "        start = int(pos)\n",
    "        num_spikes = np.random.randint(3, 6)\n",
    "        for i in range(num_spikes):\n",
    "            spike_start = start + i * np.random.randint(100, 200)\n",
    "            spike_duration = np.random.randint(40, 80)\n",
    "            magnitude = 2.0 + i * 0.5\n",
    "            cascades.append(('cascading', spike_start, spike_duration, magnitude))\n",
    "    return cascades\n",
    "\n",
    "\n",
    "def generate_flash_crowds(duration, count=50):\n",
    "    \"\"\"Instant spike + gradual decline\"\"\"\n",
    "    flash_crowds = []\n",
    "    positions = np.random.choice(duration, count, replace=False)\n",
    "    for pos in positions:\n",
    "        start = int(pos)\n",
    "        spike_duration = 10\n",
    "        decline_duration = np.random.randint(300, 600)\n",
    "        magnitude = np.random.uniform(4.0, 5.5)\n",
    "        flash_crowds.append(('flash_crowd', start, spike_duration + decline_duration, magnitude))\n",
    "    return flash_crowds\n",
    "\n",
    "\n",
    "def apply_traffic_events(t, base_traffic, all_events):\n",
    "    \"\"\"Apply all events to get final request rate\"\"\"\n",
    "    multiplier = 1.0\n",
    "    \n",
    "    for event_type, start, duration, magnitude in all_events:\n",
    "        if start <= t < start + duration:\n",
    "            progress = (t - start) / duration\n",
    "            \n",
    "            if event_type == 'sudden':\n",
    "                multiplier = max(multiplier, magnitude)\n",
    "            elif event_type == 'gradual':\n",
    "                ramp_factor = np.sin(np.pi * progress)\n",
    "                multiplier = max(multiplier, 1 + (magnitude - 1) * ramp_factor)\n",
    "            elif event_type == 'sustained':\n",
    "                multiplier = max(multiplier, magnitude * (1 + np.random.normal(0, 0.05)))\n",
    "            elif event_type == 'oscillating':\n",
    "                osc_factor = np.sin(10 * np.pi * progress)\n",
    "                multiplier = max(multiplier, 1 + magnitude * osc_factor)\n",
    "            elif event_type == 'cascading':\n",
    "                multiplier = max(multiplier, magnitude)\n",
    "            elif event_type == 'flash_crowd':\n",
    "                if progress < 0.1:\n",
    "                    multiplier = max(multiplier, magnitude)\n",
    "                else:\n",
    "                    decline_progress = (progress - 0.1) / 0.9\n",
    "                    multiplier = max(multiplier, magnitude - (magnitude - 1) * decline_progress)\n",
    "    \n",
    "    return base_traffic * multiplier\n",
    "\n",
    "print(\"‚úÖ Functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafb5c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all events\n",
    "print(\"üìã Generating traffic events...\")\n",
    "\n",
    "all_events = []\n",
    "all_events.extend(generate_sudden_spikes(TOTAL_DURATION, count=150))\n",
    "all_events.extend(generate_gradual_ramps(TOTAL_DURATION, count=80))\n",
    "all_events.extend(generate_sustained_bursts(TOTAL_DURATION, count=60))\n",
    "all_events.extend(generate_oscillating_patterns(TOTAL_DURATION, count=40))\n",
    "all_events.extend(generate_cascading_spikes(TOTAL_DURATION, count=30))\n",
    "all_events.extend(generate_flash_crowds(TOTAL_DURATION, count=50))\n",
    "\n",
    "print(f\"‚úì Total events: {len(all_events)}\")\n",
    "print(\"  - Sudden spikes: 150\")\n",
    "print(\"  - Gradual ramps: 80\")\n",
    "print(\"  - Sustained bursts: 60\")\n",
    "print(\"  - Oscillating patterns: 40\")\n",
    "print(\"  - Cascading spikes: ~120\")\n",
    "print(\"  - Flash crowds: 50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b300f298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main simulation\n",
    "print(\"\\n‚öôÔ∏è  Running simulation (this takes 3-5 minutes)...\")\n",
    "\n",
    "rows = []\n",
    "rr_index = 0\n",
    "queue = 0\n",
    "\n",
    "progress_interval = TOTAL_DURATION // 20\n",
    "\n",
    "for t in range(TOTAL_DURATION):\n",
    "    if t % progress_interval == 0:\n",
    "        progress = (t / TOTAL_DURATION) * 100\n",
    "        print(f\"  Progress: {progress:.0f}%\")\n",
    "    \n",
    "    base_req = daily_baseline_pattern(t)\n",
    "    request_rate = apply_traffic_events(t, base_req, all_events)\n",
    "    request_rate = max(5, min(500, request_rate))\n",
    "    \n",
    "    payload_kb = np.random.uniform(50, 500)\n",
    "    pod = rr_index % BASE_PODS\n",
    "    rr_index += 1\n",
    "    \n",
    "    cpu_used = min(100, (request_rate / (BASE_PODS * 15)) * 100)\n",
    "    memory_used = min(100, cpu_used * 0.8 + np.random.normal(0, 2))\n",
    "    queue = max(0, queue * 0.9 + request_rate * 0.1 - BASE_PODS * 10)\n",
    "    \n",
    "    latency = 50 + cpu_used * 0.8\n",
    "    if cpu_used > 70:\n",
    "        latency += (cpu_used - 70) * 2\n",
    "    latency += queue * 0.05\n",
    "    latency = max(30, latency + np.random.normal(0, 3))\n",
    "    \n",
    "    rows.append([\n",
    "        t, request_rate, payload_kb, queue,\n",
    "        cpu_used, memory_used, latency,\n",
    "        BASE_PODS, pod\n",
    "    ])\n",
    "\n",
    "print(\"‚úì Simulation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae12d2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "print(\"\\nüìä Creating DataFrame...\")\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\n",
    "    \"timestamp\", \"request_rate\", \"payload_size_kb\", \"queue_length\",\n",
    "    \"cpu_used_pct\", \"memory_used_pct\", \"latency_ms\",\n",
    "    \"active_pods\", \"rr_pod_index\"\n",
    "])\n",
    "\n",
    "print(f\"‚úì DataFrame created: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6cdf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "print(\"\\nüîß Engineering features...\")\n",
    "\n",
    "for window in [5, 10, 30]:\n",
    "    df[f\"req_avg_{window}s\"] = df[\"request_rate\"].rolling(window, min_periods=1).mean()\n",
    "    df[f\"req_delta_{window}s\"] = df[\"request_rate\"].diff(window).fillna(0)\n",
    "\n",
    "print(\"‚úì Rolling features created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e123e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spike Labels\n",
    "print(\"\\nüè∑Ô∏è  Creating spike labels...\")\n",
    "\n",
    "for horizon in [10, 30, 60]:\n",
    "    df[f\"future_req_{horizon}s\"] = df[\"request_rate\"].shift(-horizon)\n",
    "    \n",
    "    # Lower thresholds to catch more spikes\n",
    "    threshold = {10: 1.5, 30: 1.6, 60: 1.7}[horizon]\n",
    "    \n",
    "    df[f\"spike_{horizon}s\"] = (\n",
    "        df[f\"future_req_{horizon}s\"] > df[\"request_rate\"] * threshold\n",
    "    ).astype(int)\n",
    "    \n",
    "    spike_count = df[f\"spike_{horizon}s\"].sum()\n",
    "    spike_pct = 100 * df[f\"spike_{horizon}s\"].mean()\n",
    "    print(f\"  spike_{horizon}s: {spike_count:,} ({spike_pct:.2f}%)\")\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "print(f\"\\n‚úì Final dataset: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1bf338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CSV\n",
    "output_file = \"synthetic_k8s_load_enhanced.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nüíæ Saved: {output_file}\")\n",
    "print(f\"üìä Shape: {df.shape}\")\n",
    "print(f\"üìà Request rate range: {df['request_rate'].min():.1f} - {df['request_rate'].max():.1f}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nüìù Sample data:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309c8925",
   "metadata": {},
   "source": [
    "## ü§ñ Step 3: Train XGBoost Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c023f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import joblib\n",
    "\n",
    "print(\"üéØ Training XGBoost Models\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define features\n",
    "ALL_COLUMNS = df.columns.tolist()\n",
    "\n",
    "FEATURES = [\n",
    "    c for c in ALL_COLUMNS\n",
    "    if c not in [\n",
    "        \"timestamp\",\n",
    "        \"spike_10s\", \"spike_30s\", \"spike_60s\",\n",
    "        \"future_req_10s\", \"future_req_30s\", \"future_req_60s\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "print(f\"Features: {len(FEATURES)}\")\n",
    "print(FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f57b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models for each horizon\n",
    "HORIZONS = [10, 30, 60]\n",
    "trained_models = {}\n",
    "\n",
    "for h in HORIZONS:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training model for spike_{h}s\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    X = df[FEATURES]\n",
    "    y = df[f\"spike_{h}s\"]\n",
    "    \n",
    "    print(f\"Dataset: {X.shape[0]:,} samples, {X.shape[1]} features\")\n",
    "    print(f\"Positive samples: {y.sum():,} ({100*y.mean():.2f}%)\")\n",
    "    \n",
    "    # Split data (80/20)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, shuffle=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")\n",
    "    \n",
    "    # Train XGBoost\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"auc\",\n",
    "        tree_method=\"hist\",\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚öôÔ∏è  Training...\")\n",
    "    model.fit(X_train, y_train, verbose=False)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    print(\"\\nüìä Performance:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['No Spike', 'Spike']))\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "    \n",
    "    trained_models[h] = model\n",
    "    print(f\"\\n‚úÖ Model for spike_{h}s trained successfully!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ ALL MODELS TRAINED!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dcd844",
   "metadata": {},
   "source": [
    "## üíæ Step 4: Save Models as .pkl Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321c5067",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üíæ Saving models...\\n\")\n",
    "\n",
    "for h in HORIZONS:\n",
    "    filename = f\"xgb_spike_{h}s_enhanced.pkl\"\n",
    "    joblib.dump(trained_models[h], filename)\n",
    "    print(f\"‚úì Saved: {filename}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ SUCCESS! All models saved!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüì• Download these files:\")\n",
    "print(\"  - xgb_spike_10s_enhanced.pkl\")\n",
    "print(\"  - xgb_spike_30s_enhanced.pkl\")\n",
    "print(\"  - xgb_spike_60s_enhanced.pkl\")\n",
    "print(\"\\nüîÑ Next steps:\")\n",
    "print(\"  1. Download the .pkl files from Colab\")\n",
    "print(\"  2. Copy to: ai_load_balancer_test/models/\")\n",
    "print(\"  3. Rename (remove '_enhanced' or update main.py)\")\n",
    "print(\"  4. Re-run tests: python main.py test_cases/sudden_spike.csv\")\n",
    "print(\"  5. Watch the magic happen! üöÄ\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d14ab24",
   "metadata": {},
   "source": [
    "## üìä Step 5: Quick Verification Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a7b3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a sudden spike pattern\n",
    "print(\"üß™ Testing on synthetic sudden spike...\\n\")\n",
    "\n",
    "# Create test scenario: 40 req/s then jump to 180\n",
    "test_traffic = np.concatenate([\n",
    "    np.ones(100) * 40,  # Baseline\n",
    "    np.ones(50) * 180,  # Spike\n",
    "    np.ones(100) * 40   # Back to baseline\n",
    "])\n",
    "\n",
    "# Create features\n",
    "test_data = []\n",
    "for i, req in enumerate(test_traffic):\n",
    "    cpu = min(100, (req / (5 * 15)) * 100)\n",
    "    mem = cpu * 0.8\n",
    "    lat = 50 + cpu * 0.8\n",
    "    \n",
    "    # Rolling features\n",
    "    req_avg_5 = np.mean(test_traffic[max(0, i-5):i+1])\n",
    "    req_avg_10 = np.mean(test_traffic[max(0, i-10):i+1])\n",
    "    req_avg_30 = np.mean(test_traffic[max(0, i-30):i+1])\n",
    "    \n",
    "    test_data.append([\n",
    "        req, np.random.uniform(50, 500), 0, cpu, mem, lat, 5, i % 5,\n",
    "        req_avg_5, req - req_avg_5,\n",
    "        req_avg_10, req - req_avg_10,\n",
    "        req_avg_30, req - req_avg_30\n",
    "    ])\n",
    "\n",
    "test_df = pd.DataFrame(test_data, columns=FEATURES)\n",
    "\n",
    "# Predict\n",
    "prob_30 = trained_models[30].predict_proba(test_df)[:, 1]\n",
    "prob_60 = trained_models[60].predict_proba(test_df)[:, 1]\n",
    "\n",
    "print(\"üìà Prediction at spike point (t=100):\")\n",
    "print(f\"  Traffic: 40 ‚Üí 180 req/s\")\n",
    "print(f\"  Spike prob (30s): {prob_30[100]:.3f}\")\n",
    "print(f\"  Spike prob (60s): {prob_60[100]:.3f}\")\n",
    "\n",
    "print(\"\\nüìà Prediction BEFORE spike (t=90):\")\n",
    "print(f\"  Traffic: still at 40 req/s\")\n",
    "print(f\"  Spike prob (30s): {prob_30[90]:.3f} (should be HIGH if model learned!)\")\n",
    "print(f\"  Spike prob (60s): {prob_60[90]:.3f}\")\n",
    "\n",
    "if prob_30[90] > 0.3:\n",
    "    print(\"\\n‚úÖ Model is detecting spike BEFORE it happens! Perfect!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Model might need more tuning, but should still perform better!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Download the .pkl files and test them! üéâ\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
