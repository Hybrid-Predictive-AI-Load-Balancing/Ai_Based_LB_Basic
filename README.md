# AI Load Balancer - Comprehensive Testing Framework

A comprehensive testing framework for the AI-powered load balancer that tests model performance under extreme and varied traffic conditions with real-time visualization and detailed metrics.

## ğŸ¯ Overview

This framework tests the AI load balancer's ability to handle:
- **Gradual ramp-up/down** - Smooth traffic changes
- **Sudden spikes** - Sharp traffic increases
- **Oscillating patterns** - Periodic load variations
- **Extreme bursts** - Sustained high load
- **Low sustained traffic** - Scale-down behavior
- **Noisy irregular traffic** - Random variations
- **Multi-stage spikes** - Progressive intensity increases
- **Flash crowd** - Sudden massive spike with gradual decline
- **Cascading spikes** - Sequential spikes getting closer together

## ğŸ“ Directory Structure

```
ai_load_balancer_test/
â”œâ”€â”€ models/                      # Pre-trained XGBoost models
â”‚   â”œâ”€â”€ xgb_spike_10s.pkl
â”‚   â”œâ”€â”€ xgb_spike_30s.pkl
â”‚   â””â”€â”€ xgb_spike_60s.pkl
â”œâ”€â”€ test_cases/                  # Generated test CSV files
â”‚   â”œâ”€â”€ gradual_ramp_up_down.csv
â”‚   â”œâ”€â”€ sudden_spike.csv
â”‚   â”œâ”€â”€ oscillating_pattern.csv
â”‚   â”œâ”€â”€ extreme_burst.csv
â”‚   â”œâ”€â”€ low_sustained_traffic.csv
â”‚   â”œâ”€â”€ noisy_irregular.csv
â”‚   â”œâ”€â”€ multi_stage_spike.csv
â”‚   â”œâ”€â”€ flash_crowd.csv
â”‚   â””â”€â”€ cascading_spikes.csv
â”œâ”€â”€ results/                     # Test results and reports
â”‚   â”œâ”€â”€ *_detailed_log.csv      # Per-test detailed logs
â”‚   â”œâ”€â”€ *_state_changes.csv     # Scaling action logs
â”‚   â”œâ”€â”€ *_plot.png              # Visualization plots
â”‚   â””â”€â”€ SUMMARY_REPORT_*.csv    # Aggregate summary
â”œâ”€â”€ main.py                      # Main test runner
â”œâ”€â”€ autoscaler.py               # Enhanced autoscaler with tracking
â”œâ”€â”€ simulator.py                # Enhanced simulator with metrics
â”œâ”€â”€ visualize.py                # Real-time visualization
â”œâ”€â”€ generate_test_cases.py      # Test case generator
â””â”€â”€ requirements.txt            # Python dependencies
```

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Generate Test Cases (Already Done)

Test cases are pre-generated, but you can regenerate them:

```bash
python generate_test_cases.py
```

### 3. Run All Tests

```bash
python main.py
```

### 4. Run Specific Test

```bash
python main.py test_cases/sudden_spike.csv
```

## ğŸ“Š Features

### Real-Time Visualization

The framework provides comprehensive real-time visualization with 5 plots:

1. **Traffic Rate** - Request rate over time with spike indicators
2. **Pod Count** - Active pods with scale action markers (â–² up, â–¼ down)
3. **Latency** - Response latency with threshold violations highlighted
4. **CPU & Memory** - Resource utilization with threshold lines
5. **Spike Probabilities** - Model predictions for 30s and 60s spikes

### State Change Logging

Every scaling action is logged with:
- Timestamp
- Action type (SCALE_UP, SCALE_DOWN, HOLD)
- Reason for decision
- Current metrics (CPU, latency, predictions)
- Spike description

### Comprehensive Metrics

Each test tracks:
- **Scaling metrics**: Scale-ups, scale-downs, hold rate
- **Performance metrics**: CPU usage, latency, violations
- **Efficiency metrics**: Underutilization, overload frequency
- **Pod usage**: Average, min, max pod count

## ğŸ“ˆ Output Files

### Detailed Logs (`*_detailed_log.csv`)
Complete time-series data for every simulation step:
- Request rate, pods, CPU, memory, latency
- Model predictions (30s, 60s probabilities)
- Actions and reasons
- Spike descriptions
- Violations

### State Changes (`*_state_changes.csv`)
Log of all scaling actions:
- When actions occurred
- Why decisions were made
- System state at decision time

### Plots (`*_plot.png`)
High-resolution visualization of entire test run

### Summary Report (`SUMMARY_REPORT_*.csv`)
Aggregate statistics across all tests for comparison

## âš™ï¸ Configuration

Edit the configuration section in `main.py`:

```python
ROWS_PER_SECOND = 10     # Simulation speed (steps/second)
SLEEP_TIME = 1           # Real seconds between updates
SAVE_PLOTS = True        # Save plots after each test
CONSOLE_LOG_INTERVAL = 50  # Console update frequency
```

## ğŸ® Test Case Descriptions

| Test Case | Description | Purpose |
|-----------|-------------|---------|
| `gradual_ramp_up_down` | Smooth sine wave traffic pattern | Test smooth scaling behavior |
| `sudden_spike` | Multiple sharp spikes | Test reaction time and prediction |
| `oscillating_pattern` | Periodic load variations | Test handling of rhythmic patterns |
| `extreme_burst` | Sustained 200 req/s load | Test maximum capacity handling |
| `low_sustained_traffic` | Very low traffic (~25 req/s) | Test aggressive scale-down |
| `noisy_irregular` | High-frequency random changes | Test noise filtering capability |
| `multi_stage_spike` | Progressive spikes (80â†’200 req/s) | Test adaptive scaling |
| `flash_crowd` | Instant spike + gradual decline | Test viral load scenarios |
| `cascading_spikes` | Sequential spikes getting closer | Stress test recovery time |

## ğŸ“Š Understanding Results

### Good Performance Indicators:
âœ… Low latency violation rate (<5%)
âœ… Low CPU overload rate (<10%)
âœ… Balanced scale-up/down ratio
âœ… Quick response to spikes (visible in plots)
âœ… Minimal underutilization

### Warning Signs:
âš ï¸ High latency violations (>10%)
âš ï¸ Frequent CPU overload (>20%)
âš ï¸ Excessive scaling actions (thrashing)
âš ï¸ Slow spike response

## ğŸ”§ Customization

### Adding New Test Cases

Edit `generate_test_cases.py` and add a new function:

```python
def my_custom_test(size=2000):
    """My custom traffic pattern"""
    request_rate = # ... your pattern logic
    df = generate_base_features(request_rate, size)
    return df
```

Then add it to the `test_cases` dictionary in `generate_all_test_cases()`.

### Tuning Autoscaler

Edit `autoscaler.py` to modify scaling thresholds:

```python
def __init__(self, min_pods=2, max_pods=20, cooldown=10):
    # Adjust these parameters
```

### Adjusting Thresholds

Edit scaling logic in `autoscaler.py`:

```python
if prob_30s >= 0.35:  # Change threshold here
    # Scale up logic
```

## ğŸ“ Example Console Output

```
ğŸ§ª RUNNING TEST: sudden_spike
======================================================================
ğŸ“Š Test data: 2000 time steps
ğŸ“ˆ Traffic range: 40.0 - 180.0 req/s

â–¶ï¸  Starting simulation... (Press Ctrl+C to stop)
â±ï¸  Update rate: 10 steps/second

âš¡ t=0400 | SCALE_UP     | Pods: 7  | CPU:  89.2% | Latency: 167.3ms | High spike probability (30s:0.78, 60s:0.65)
âš¡ t=0450 | SCALE_DOWN   | Pods: 6  | CPU:  25.1% | Latency:  70.1ms | Low spike probability (0.12) + Low CPU (25.1%)
â³ Progress: 500/2000 steps (25.0%) | Pods: 6 | CPU: 48.3% | Latency: 88.7ms

ğŸ“Š TEST RESULTS: sudden_spike
======================================================================
â±ï¸  Elapsed time: 205.43s
ğŸ“ˆ Steps completed: 2000/2000 (100.0%)

ğŸ¯ AUTOSCALER STATISTICS:
   Total scale-ups:     42 ( 2.1%)
   Total scale-downs:   38 ( 1.9%)
   Total holds:       1920 (96.0%)

âš¡ PERFORMANCE METRICS:
   Avg CPU:              52.3%
   Max CPU:              96.8%
   Avg Latency:          89.7 ms
   Max Latency:         245.2 ms
   Latency violations:     15 ( 0.8%)
   CPU overload:          23 ( 1.2%)
   Underutilization:     342 (17.1%)

ğŸ”¢ POD USAGE:
   Average pods:           5.8
   Min pods:                 2
   Max pods:                12
```

## ğŸ¯ Goals

This testing framework helps evaluate:
1. **Response Time** - How quickly does the system react to spikes?
2. **Prediction Accuracy** - Are spikes detected before they cause issues?
3. **Scaling Efficiency** - Is the system avoiding over/under-scaling?
4. **Resource Utilization** - Is CPU/memory usage optimal?
5. **SLA Compliance** - Are latency thresholds maintained?
6. **Stability** - Does the system avoid thrashing?

## ğŸ“ Support

For issues or questions, refer to the main project documentation or check the generated logs in `results/` for detailed information.

---

**Note**: This is a testing framework. Actual production deployment would require additional monitoring, alerting, and integration with Kubernetes or similar orchestration platforms.
